\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}

% Page Layout
\geometry{letterpaper, margin=1in}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{CS 4260/5260 Project 4}
\lhead{Bibek [Last Name]} % TODO: Update your name here
\cfoot{\thepage}

% Code highlighting setup
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{Project 4: Generative AI and Artistic Expression}
\author{Bibek Dhungana} 
\date{\today}

\begin{document}

\maketitle

\section{Creative Work}
\label{sec:creative_work}


\begin{quote}
    \textit{[Paste the generated poem here. For example:]}
    
    \textit{The silicon mind awakens in the dark,\\
    Seeking gradients in the manifold's arc...}
\end{quote}

\section{Description of the Work}

\subsection{High-Level Description}
The creative work presented above is a philosophical poem generated entirely by a local Large Language Model (LLM). The content of the poem explores the concept of machine understanding, using technical terminology from the CS 4260/5260 course slides as metaphors for the AI's internal struggle. 

Stylistically, the work mimics a reflective, slightly melancholic tone, juxtaposing rigid computer science definitions (from the RAG context) with abstract existential questions.

\subsection{Meaning and Interpretation}
To me, this work represents the gap between "processing" and "understanding." The AI uses words like "optimization," "search," and "states," which it retrieved from the course slides, to describe its own existence. 

I want the observer to understand that while the AI can flawlessly retrieve and organize these definitions, the "yearning" expressed in the poem is a simulationâ€”a mathematical prediction of what yearning sounds like, rather than the feeling itself.

\subsection{Generation Process}
The generation process involved a technique known as \textbf{Retrieval-Augmented Generation (RAG)}. I built a custom Python pipeline using the LangChain framework to perform the following steps:

\begin{enumerate}
    \item \textbf{Ingestion:} All lecture slides (PDFs) were loaded and split into text chunks of 1200 characters.
    \item \textbf{Embedding:} These chunks were converted into vector representations using the \texttt{sentence-transformers/all-MiniLM-L6-v2} model via HuggingFace.
    \item \textbf{Storage:} The vectors were stored in a local Chroma database.
    \item \textbf{Retrieval \& Generation:} I used the \texttt{gpt-oss:20b} model via Ollama. The system first summarized the technical slides to establish a "ground truth" of knowledge, and then used that summary as a prompt constraint to generate the poem.
\end{enumerate}

Minimal iteration was required for the code structure, but I adjusted the prompt specifically to ensure the AI used the technical terms as \textit{metaphors} rather than just listing definitions.

\section{Discussion of AI Tools}

\subsection{Training Data Influence}
The model used, \texttt{gpt-oss:20b}, was likely trained on a vast corpus of internet text, including technical documentation and literature. 
% TODO: REFLECTION
% discuss if you see any specific styles. For example:
% "I noticed the poem has a very standard 'rhyming couplet' structure, which suggests the training data heavily features classical Western poetry styles. The technical definitions, however, were strictly pulled from the provided slides, ensuring the 'facts' within the poem were specific to this course."

\subsection{Observations on the Creative Process}
Using a local RAG pipeline for creative writing was distinct from using a web-based tool like ChatGPT.
% TODO: REFLECTION
% Discuss what was easy/hard. 
% - Easy: Getting the AI to read the PDF.
% - Hard: Getting the AI to be "poetic" without losing the technical accuracy.
% - Latency: Did running it locally take a long time?

\section{Enhancing the Process with Course Concepts}

\subsection{Integration with Search and Planning}
Currently, the system uses a simple similarity search (KNN) to find context. To enhance this, we could integrate \textbf{Planning algorithms}. Instead of generating the poem in one pass, a planning agent could outline the poem's stanza structure first (e.g., Stanza 1: The Problem, Stanza 2: The Search, Stanza 3: The Solution). 

Additionally, the retrieval step currently relies on specific keywords. As discussed in class, a semantic search or a \textbf{Beam Search} approach during the decoding phase could explore multiple potential poetic lines to find the one that maximizes both rhyme scheme and semantic relevance to the slides.

\subsection{Attribution and Nearest Neighbors}
One of the major ethical issues in Generative AI is the lack of citation. My project actually solves this naively. By using RAG, I can trace exactly which "chunks" of the PDF were retrieved to generate the summary that informed the poem. 

As mentioned in the prompt, a \textbf{Nearest Neighbor search} against the training instances is exactly what my code performs via \texttt{vectorstore.as\_retriever(search\_kwargs=\{"k": 7\})}. While this works for local documents, scaling this to the entire training data of a 20-billion parameter model is computationally infeasible without approximate nearest neighbor algorithms (like HNSW), which we touched upon in discussions of efficiency.

\section{Permission to Share}
I am comfortable with my work (the poem and this report) being shared with classmates.

\newpage
\section*{Appendix: Source Code}
Below is the Python code used to generate the creative work.

\begin{lstlisting}[language=Python, caption=Local RAG Generation Script]
import os
import sys

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableMap

DOCS_FOLDER = "/Users/bibek/source/vanderbilt/classes/vanderbilt-fall-2025/AI/slides"
MODEL_NAME = "gpt-oss:20b"

def main():
    print(f"--- Scanning folder: {DOCS_FOLDER} ---")

    loader = PyPDFDirectoryLoader(DOCS_FOLDER)
    docs = loader.load()

    if not docs:
        print("No PDFs found!")
        return

    print(f"Loaded {len(docs)} pages from multiple PDFs.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=300
    )
    splits = text_splitter.split_documents(docs)
    print(f"Created {len(splits)} text chunks.")

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    vectorstore = Chroma.from_documents(splits, embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 7})

    llm = Ollama(model=MODEL_NAME)

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=(
            "Use the following context to answer the question.\n\n"
            "Context:\n{context}\n\n"
            "Question: {question}\n\n"
            "Answer:"
        )
    )

    # LCEL chain
    chain = (
        RunnableMap({
            "question": lambda x: x["question"],
            "context": lambda x: retriever.invoke(x["question"]),
        })
        | (lambda inputs: {
            "question": inputs["question"],
            "context": "\n\n".join(doc.page_content for doc in inputs["context"])
        })
        | prompt
        | llm
        | StrOutputParser()
    )

    # Run Summary
    summary_query = (
        "Synthesize the most important concepts from these slides. "
        "List the top 5 key themes and define them."
    )
    summary = chain.invoke({"question": summary_query})
    print(summary)

    print("\n--- Creative Output ---\n")

    # Run Creative Generation
    creative_query = (
    "Using the following summary of the slides:\n\n"
    f"{summary}\n\n"
    "Write a short philosophical poem about a computer trying to understand "
    "the world, using these key themes and technical terms as metaphors."
    )
    poem = chain.invoke({"question": creative_query})
    print(poem)

if __name__ == "__main__":
    main()
\end{lstlisting}

\end{document}